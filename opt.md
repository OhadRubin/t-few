|     | module            | c_name               | layer                                                                        | should_modify   | is_linear   |
|----:|:------------------|:---------------------|:-----------------------------------------------------------------------------|:----------------|:------------|
|   0 | OPTModel'>        | decoder              | <class 'transformers.models.opt.modeling_opt.OPTDecoder'>                    | False           | False       |
|   1 | OPTDecoder'>      | embed_tokens         | <class 'torch.nn.modules.sparse.Embedding'>                                  | False           | False       |
|   2 | OPTDecoder'>      | embed_positions      | <class 'transformers.models.opt.modeling_opt.OPTLearnedPositionalEmbedding'> | False           | False       |
|   3 | OPTDecoder'>      | project_out          | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|   4 | OPTDecoder'>      | project_in           | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|   5 | OPTDecoder'>      | layers               | <class 'torch.nn.modules.container.ModuleList'>                              | False           | False       |
|   6 | ModuleList'>      | 0                    | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|   7 | ModuleList'>      | 1                    | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|   8 | ModuleList'>      | 2                    | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|   9 | ModuleList'>      | 3                    | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  10 | ModuleList'>      | 4                    | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  11 | ModuleList'>      | 5                    | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  12 | ModuleList'>      | 6                    | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  13 | ModuleList'>      | 7                    | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  14 | ModuleList'>      | 8                    | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  15 | ModuleList'>      | 9                    | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  16 | ModuleList'>      | 10                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  17 | ModuleList'>      | 11                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  18 | ModuleList'>      | 12                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  19 | ModuleList'>      | 13                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  20 | ModuleList'>      | 14                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  21 | ModuleList'>      | 15                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  22 | ModuleList'>      | 16                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  23 | ModuleList'>      | 17                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  24 | ModuleList'>      | 18                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  25 | ModuleList'>      | 19                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  26 | ModuleList'>      | 20                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  27 | ModuleList'>      | 21                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  28 | ModuleList'>      | 22                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  29 | ModuleList'>      | 23                   | <class 'transformers.models.opt.modeling_opt.OPTDecoderLayer'>               | False           | False       |
|  30 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
|  31 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
|  32 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  33 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  34 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  35 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  36 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  37 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  38 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  39 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  40 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
|  41 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
|  42 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  43 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  44 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  45 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  46 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  47 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  48 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  49 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  50 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
|  51 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
|  52 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  53 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  54 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  55 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  56 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  57 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  58 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  59 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  60 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
|  61 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
|  62 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  63 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  64 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  65 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  66 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  67 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  68 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  69 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  70 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
|  71 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
|  72 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  73 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  74 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  75 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  76 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  77 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  78 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  79 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  80 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
|  81 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
|  82 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  83 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  84 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  85 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  86 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  87 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  88 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  89 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  90 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
|  91 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
|  92 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  93 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  94 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  95 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
|  96 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  97 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  98 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
|  99 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 100 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 101 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 102 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 103 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 104 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 105 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 106 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 107 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 108 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 109 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 110 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 111 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 112 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 113 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 114 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 115 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 116 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 117 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 118 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 119 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 120 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 121 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 122 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 123 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 124 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 125 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 126 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 127 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 128 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 129 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 130 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 131 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 132 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 133 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 134 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 135 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 136 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 137 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 138 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 139 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 140 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 141 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 142 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 143 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 144 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 145 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 146 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 147 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 148 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 149 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 150 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 151 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 152 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 153 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 154 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 155 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 156 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 157 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 158 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 159 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 160 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 161 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 162 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 163 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 164 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 165 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 166 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 167 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 168 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 169 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 170 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 171 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 172 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 173 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 174 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 175 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 176 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 177 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 178 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 179 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 180 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 181 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 182 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 183 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 184 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 185 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 186 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 187 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 188 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 189 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 190 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 191 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 192 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 193 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 194 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 195 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 196 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 197 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 198 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 199 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 200 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 201 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 202 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 203 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 204 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 205 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 206 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 207 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 208 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 209 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 210 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 211 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 212 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 213 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 214 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 215 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 216 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 217 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 218 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 219 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 220 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 221 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 222 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 223 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 224 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 225 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 226 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 227 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 228 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 229 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 230 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 231 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 232 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 233 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 234 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 235 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 236 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 237 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 238 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 239 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 240 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 241 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 242 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 243 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 244 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 245 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 246 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 247 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 248 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 249 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 250 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 251 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 252 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 253 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 254 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 255 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 256 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 257 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 258 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 259 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 260 | OPTDecoderLayer'> | self_attn            | <class 'transformers.models.opt.modeling_opt.OPTAttention'>                  | False           | False       |
| 261 | OPTDecoderLayer'> | activation_fn        | <class 'torch.nn.modules.activation.ReLU'>                                   | False           | False       |
| 262 | OPTDecoderLayer'> | self_attn_layer_norm | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 263 | OPTDecoderLayer'> | fc1                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 264 | OPTDecoderLayer'> | fc2                  | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 265 | OPTDecoderLayer'> | final_layer_norm     | <class 'torch.nn.modules.normalization.LayerNorm'>                           | False           | False       |
| 266 | OPTAttention'>    | k_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 267 | OPTAttention'>    | v_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 268 | OPTAttention'>    | q_proj               | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |
| 269 | OPTAttention'>    | out_proj             | <class 'torch.nn.modules.linear.Linear'>                                     | False           | True        |