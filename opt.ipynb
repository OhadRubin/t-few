{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 't-few'\n",
      "/content/drive/MyDrive/drive_workspace/t-few\n"
     ]
    }
   ],
   "source": [
    "%cd t-few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install omegaconf\n",
    "# !pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from src.data import FinetuneDataModule, get_dataset_reader, PretrainDataModule\n",
    "from src.models.EncoderDecoder import EncoderDecoder\n",
    "from src.models.modify_model import modify_transformer\n",
    "from src.utils.Config import Config\n",
    "from src.utils.util import ParseKwargs, set_seeds\n",
    "from omegaconf import OmegaConf\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from src.models.lora import LoRALinear\n",
    "import torch.nn as nn\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def maybe_modify(config,module,c_name,m_name,layer,debug=False):\n",
    "    should_modify = bool(re.fullmatch(config.lora_modules, m_name)) and bool(re.fullmatch(config.lora_layers, c_name)) \n",
    "                    \n",
    "    if not debug:\n",
    "        if should_modify:\n",
    "            setattr(\n",
    "                module,\n",
    "                c_name,\n",
    "                LoRALinear(layer, config.lora_rank, config.lora_scaling_rank, config.lora_init_scale),\n",
    "            )\n",
    "    else:\n",
    "        return dict(\n",
    "            module=m_name,\n",
    "            c_name=c_name,layer=type(layer),should_modify=should_modify,is_linear = isinstance(layer, nn.Linear))\n",
    "\n",
    "def lora_modify(transformer, config,debug=False):\n",
    "    d_list = []\n",
    "    for m_name, module in dict(transformer.named_modules()).items():\n",
    "        for c_name, layer in dict(module.named_children()).items():\n",
    "            a = maybe_modify(config,module,c_name,m_name,layer,debug)\n",
    "            d_list.append(a)\n",
    "\n",
    "                    \n",
    " \n",
    "    return transformer,d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset poem_sentiment (/root/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c0868621b14c8182ac79ab6aa26b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 892\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 105\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"poem_sentiment\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function encode_batch at 0x7ff631704ef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1401038e2b8746818d43c2708327b2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b79524a867c453a8d672a4e01843b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf697d06b8c4c15840a9f5e1afb4fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def encode_batch(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  encoding = tokenizer(batch[\"verse_text\"])\n",
    "  # For language modeling the labels need to be the input_ids\n",
    "  #encoding[\"labels\"] = encoding[\"input_ids\"]\n",
    "  return encoding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "# The GPT-2 tokenizer does not have a padding token. In order to process the data \n",
    "# in batches we set one here \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "column_names = dataset[\"train\"].column_names\n",
    "dataset = dataset.map(encode_batch, remove_columns=column_names, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b519a29f2841d2829ce8e55139503d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f82910921c4292adbe6e861f02383c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c617cadee34e7d98f93bda4d59d69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 50\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "  # Concatenate all texts.\n",
    "  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "  total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "  # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "  # customize this part to your needs.\n",
    "  total_length = (total_length // block_size) * block_size\n",
    "  # Split by chunks of max_len.\n",
    "  result = {\n",
    "    k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "  }\n",
    "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "  return result\n",
    "\n",
    "dataset = dataset.map(group_texts,batched=True,)\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
    "config  = OmegaConf.create({\n",
    "    \"lora_scaling_rank\": 1,\n",
    "    \"lora_rank\": 0,\n",
    "    \"lora_init_scale\": 0.0,\n",
    "    \"lora_modules\": \".*\",\n",
    "    \"lora_layers\": \"k_proj|v_proj|fc2\",\n",
    "    \"trainable_param_names\": \".*lora_b.*\",\n",
    "    \"model_modifier\": \"lora\",\n",
    "    \"optimizer\":\"adamw\",\n",
    "    \"lr\": 3e-3,\n",
    "    \"num_steps\": 1000,\n",
    "    \"scheduler\":\"cosine_annealing\",\n",
    "    \"warmup_ratio\":0.1,\n",
    "    \"weight_decay\":0,\n",
    "    })\n",
    "lora_config  = OmegaConf.create({\n",
    "    \"lora_scaling_rank\": 0,\n",
    "    \"lora_rank\": 4,\n",
    "    \"lora_init_scale\": 0.01,\n",
    "    \"lora_layers\": \"k_proj|v_proj|out_proj|fc1|fc2\",\n",
    "    \"model_modifier\": \"lora\",\n",
    "    \"lora_modules\": \".*\",\n",
    "    # \"lora_layers\": \"q|k|v|o|w.*\",\n",
    "    \"trainable_param_names\": \".*layer_norm.*|.*lora_[ab].*\",\n",
    "    \"optimizer\":\"adamw\",\n",
    "    \"lr\": 3e-3,\n",
    "    \"num_steps\": 1000,\n",
    "    \"scheduler\":\"cosine_annealing\",\n",
    "    \"warmup_ratio\":0.1,\n",
    "    \"weight_decay\":0,\n",
    "    })\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
    "model,_ = lora_modify(model,lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.get_optimizer import get_optimizer\n",
    "from src.utils.get_scheduler import get_scheduler\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
      "      (layers): ModuleList(\n",
      "        (0): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (12): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (13): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (14): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (15): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (16): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (17): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (18): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (19): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (20): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (21): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (22): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (23): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (v_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (out_proj): LoRALinear(in_features=2048, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): LoRALinear(in_features=2048, out_features=8192, bias=True, rank=4, scaling_rank=0)\n",
      "          (fc2): LoRALinear(in_features=8192, out_features=2048, bias=True, rank=4, scaling_rank=0)\n",
      "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer,_ = get_optimizer(model,config)\n",
    "scheduler = get_scheduler(optimizer,config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./examples\", \n",
    "  do_train=True,\n",
    "  remove_unused_columns=False,\n",
    "  per_device_train_batch_size=12,\n",
    "  learning_rate=5e-4,\n",
    "  num_train_epochs=50,\n",
    "  logging_steps=20\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "        # optimizers=(optimizer,scheduler),\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"], \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 194\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='850' max='850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [850/850 10:27, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>11.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>9.719100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.927400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.368700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5.987500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.709400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>5.531400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>5.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>5.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>5.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>5.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>4.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>4.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>4.903600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>4.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>4.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>4.837800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>4.822600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>4.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.786200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>4.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>4.790600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>4.774800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>4.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.776500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>4.749400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>4.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>4.715500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>4.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.702800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>4.738900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>4.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>4.702300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>4.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>4.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>4.681000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./examples/checkpoint-500\n",
      "Configuration saved in ./examples/checkpoint-500/config.json\n",
      "Model weights saved in ./examples/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./examples/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./examples/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=850, training_loss=5.346603667315315, metrics={'train_runtime': 628.3977, 'train_samples_per_second': 15.436, 'train_steps_per_second': 1.353, 'total_flos': 3526175784960000.0, 'train_loss': 5.346603667315315, 'epoch': 50.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainOutput' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6240/3096185379.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTrainOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6.74534691772461\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train_runtime'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m506.268\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_samples_per_second'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m19.16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_steps_per_second'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2.469\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'total_flos'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3518309007360000.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m6.74534691772461\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m50.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TrainOutput' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "TrainOutput(global_step=1250, training_loss=6.74534691772461, metrics={'train_runtime': 506.268, 'train_samples_per_second': 19.16, 'train_steps_per_second': 2.469, 'total_flos': 3518309007360000.0, 'train_loss': 6.74534691772461, 'epoch': 50.0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
