|     | module                  | c_name                  | layer                                                              | should_modify   |
|----:|:------------------------|:------------------------|:-------------------------------------------------------------------|:----------------|
|   0 | T5Model'>               | shared                  | <class 'torch.nn.modules.sparse.Embedding'>                        | False           |
|   1 | T5Model'>               | encoder                 | <class 'transformers.models.t5.modeling_t5.T5Stack'>               | False           |
|   2 | T5Model'>               | decoder                 | <class 'transformers.models.t5.modeling_t5.T5Stack'>               | False           |
|   3 | T5Stack'>               | embed_tokens            | <class 'torch.nn.modules.sparse.Embedding'>                        | False           |
|   4 | T5Stack'>               | block                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
|   5 | T5Stack'>               | final_layer_norm        | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
|   6 | T5Stack'>               | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|   7 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
|   8 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
|   9 | ModuleList'>            | 2                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
|  10 | ModuleList'>            | 3                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
|  11 | ModuleList'>            | 4                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
|  12 | ModuleList'>            | 5                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
|  13 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
|  14 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
|  15 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
|  16 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
|  17 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
|  18 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  19 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  20 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
|  21 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
|  22 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  23 | T5Attention'>           | relative_attention_bias | <class 'torch.nn.modules.sparse.Embedding'>                        | False           |
|  24 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
|  25 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
|  26 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  27 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  28 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  29 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  30 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |
|  31 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
|  32 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
|  33 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
|  34 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
|  35 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
|  36 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  37 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  38 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
|  39 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
|  40 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  41 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
|  42 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
|  43 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  44 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  45 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  46 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  47 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |
|  48 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
|  49 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
|  50 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
|  51 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
|  52 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
|  53 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  54 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  55 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
|  56 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
|  57 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  58 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
|  59 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
|  60 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  61 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  62 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  63 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  64 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |
|  65 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
|  66 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
|  67 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
|  68 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
|  69 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
|  70 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  71 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  72 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
|  73 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
|  74 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  75 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
|  76 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
|  77 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  78 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  79 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  80 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  81 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |
|  82 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
|  83 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
|  84 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
|  85 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
|  86 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
|  87 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  88 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  89 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
|  90 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
|  91 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  92 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
|  93 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
|  94 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  95 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  96 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
|  97 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
|  98 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |
|  99 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
| 100 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
| 101 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
| 102 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 103 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 104 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 105 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 106 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 107 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 108 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 109 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
| 110 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 111 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 112 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 113 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 114 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 115 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |
| 116 | T5Stack'>               | embed_tokens            | <class 'torch.nn.modules.sparse.Embedding'>                        | False           |
| 117 | T5Stack'>               | block                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
| 118 | T5Stack'>               | final_layer_norm        | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 119 | T5Stack'>               | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 120 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
| 121 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
| 122 | ModuleList'>            | 2                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
| 123 | ModuleList'>            | 3                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
| 124 | ModuleList'>            | 4                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
| 125 | ModuleList'>            | 5                       | <class 'transformers.models.t5.modeling_t5.T5Block'>               | False           |
| 126 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
| 127 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
| 128 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerCrossAttention'> | False           |
| 129 | ModuleList'>            | 2                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
| 130 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 131 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 132 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 133 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 134 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 135 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 136 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 137 | T5Attention'>           | relative_attention_bias | <class 'torch.nn.modules.sparse.Embedding'>                        | False           |
| 138 | T5LayerCrossAttention'> | EncDecAttention         | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 139 | T5LayerCrossAttention'> | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 140 | T5LayerCrossAttention'> | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 141 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 142 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 143 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 144 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 145 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
| 146 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 147 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 148 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 149 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 150 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 151 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |
| 152 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
| 153 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
| 154 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerCrossAttention'> | False           |
| 155 | ModuleList'>            | 2                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
| 156 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 157 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 158 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 159 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 160 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 161 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 162 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 163 | T5LayerCrossAttention'> | EncDecAttention         | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 164 | T5LayerCrossAttention'> | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 165 | T5LayerCrossAttention'> | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 166 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 167 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 168 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 169 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 170 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
| 171 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 172 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 173 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 174 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 175 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 176 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |
| 177 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
| 178 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
| 179 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerCrossAttention'> | False           |
| 180 | ModuleList'>            | 2                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
| 181 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 182 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 183 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 184 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 185 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 186 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 187 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 188 | T5LayerCrossAttention'> | EncDecAttention         | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 189 | T5LayerCrossAttention'> | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 190 | T5LayerCrossAttention'> | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 191 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 192 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 193 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 194 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 195 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
| 196 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 197 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 198 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 199 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 200 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 201 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |
| 202 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
| 203 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
| 204 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerCrossAttention'> | False           |
| 205 | ModuleList'>            | 2                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
| 206 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 207 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 208 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 209 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 210 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 211 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 212 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 213 | T5LayerCrossAttention'> | EncDecAttention         | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 214 | T5LayerCrossAttention'> | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 215 | T5LayerCrossAttention'> | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 216 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 217 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 218 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 219 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 220 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
| 221 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 222 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 223 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 224 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 225 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 226 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |
| 227 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
| 228 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
| 229 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerCrossAttention'> | False           |
| 230 | ModuleList'>            | 2                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
| 231 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 232 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 233 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 234 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 235 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 236 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 237 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 238 | T5LayerCrossAttention'> | EncDecAttention         | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 239 | T5LayerCrossAttention'> | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 240 | T5LayerCrossAttention'> | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 241 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 242 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 243 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 244 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 245 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
| 246 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 247 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 248 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 249 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 250 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 251 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |
| 252 | T5Block'>               | layer                   | <class 'torch.nn.modules.container.ModuleList'>                    | False           |
| 253 | ModuleList'>            | 0                       | <class 'transformers.models.t5.modeling_t5.T5LayerSelfAttention'>  | False           |
| 254 | ModuleList'>            | 1                       | <class 'transformers.models.t5.modeling_t5.T5LayerCrossAttention'> | False           |
| 255 | ModuleList'>            | 2                       | <class 'transformers.models.t5.modeling_t5.T5LayerFF'>             | False           |
| 256 | T5LayerSelfAttention'>  | SelfAttention           | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 257 | T5LayerSelfAttention'>  | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 258 | T5LayerSelfAttention'>  | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 259 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 260 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 261 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 262 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 263 | T5LayerCrossAttention'> | EncDecAttention         | <class 'transformers.models.t5.modeling_t5.T5Attention'>           | False           |
| 264 | T5LayerCrossAttention'> | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 265 | T5LayerCrossAttention'> | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 266 | T5Attention'>           | q                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 267 | T5Attention'>           | k                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 268 | T5Attention'>           | v                       | <class 'src.models.lora.LoRALinear'>                               | True            |
| 269 | T5Attention'>           | o                       | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 270 | T5LayerFF'>             | DenseReluDense          | <class 'transformers.models.t5.modeling_t5.T5DenseActDense'>       | False           |
| 271 | T5LayerFF'>             | layer_norm              | <class 'transformers.models.t5.modeling_t5.T5LayerNorm'>           | False           |
| 272 | T5LayerFF'>             | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 273 | T5DenseActDense'>       | wi                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 274 | T5DenseActDense'>       | wo                      | <class 'torch.nn.modules.linear.Linear'>                           | False           |
| 275 | T5DenseActDense'>       | dropout                 | <class 'torch.nn.modules.dropout.Dropout'>                         | False           |
| 276 | T5DenseActDense'>       | act                     | <class 'torch.nn.modules.activation.ReLU'>                         | False           |